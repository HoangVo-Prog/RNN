{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d18b44e-52dd-42ff-9827-6442a7f11550",
   "metadata": {
    "id": "2d18b44e-52dd-42ff-9827-6442a7f11550"
   },
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bba191f3-c33e-4374-b824-1d7baa00866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.11\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.16.0+cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!python --version\n",
    "import torchtext\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "OMwsD7f6LFM1",
   "metadata": {
    "id": "OMwsD7f6LFM1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function torchtext.data.utils._basic_english_normalize(line)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "069c60c4-ccb8-403c-b16a-7266f9a1ff6a",
   "metadata": {
    "id": "069c60c4-ccb8-403c-b16a-7266f9a1ff6a",
    "outputId": "0f5ffdf3-fcff-404c-8a5d-710b34d109da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'learning', 'ai']\n",
      "['ai', 'is', 'a', 'cs', 'topic']\n"
     ]
    }
   ],
   "source": [
    "sample1 = 'We are learning AI'\n",
    "sample2 = 'AI is a CS topic'\n",
    "\n",
    "# Define tokenizer function\n",
    "sample1_tokens = tokenizer(sample1)\n",
    "sample2_tokens = tokenizer(sample2)\n",
    "\n",
    "print(sample1_tokens)\n",
    "print(sample2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700ffe54-f581-45aa-98a3-441c1dc85b43",
   "metadata": {
    "id": "700ffe54-f581-45aa-98a3-441c1dc85b43"
   },
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "sample1 = 'We are learning AI'\n",
    "sample2 = 'AI is a CS topic'\n",
    "data = [sample1, sample2]\n",
    "\n",
    "# Create a function to yield list of tokens\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "def yield_tokens(examples):\n",
    "    for text in examples:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "# Create vocabulary\n",
    "vocab_size = 8\n",
    "vocab = build_vocab_from_iterator(yield_tokens(data),\n",
    "                                  max_tokens=vocab_size,\n",
    "                                  specials=[\"<unk>\",\n",
    "                                            \"<pad>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e337b92e-bf41-4d10-afbe-096418095da4",
   "metadata": {
    "id": "e337b92e-bf41-4d10-afbe-096418095da4",
    "outputId": "cc370928-be81-4461-e42f-2a6e9e63fb92"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 0,\n",
       " '<pad>': 1,\n",
       " 'ai': 2,\n",
       " 'a': 3,\n",
       " 'is': 6,\n",
       " 'are': 4,\n",
       " 'learning': 7,\n",
       " 'cs': 5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9f1d59f-c4b5-42d1-bb56-54b51ea50c33",
   "metadata": {
    "id": "b9f1d59f-c4b5-42d1-bb56-54b51ea50c33",
    "outputId": "36cad79f-17e5-4621-fe91-cc5f7a8b1595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 'are', 'learning', 'ai']\n",
      "[0, 4, 7, 2]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sample1)\n",
    "print(tokens)\n",
    "\n",
    "sample1_tokens = [vocab[token] for token in tokens]\n",
    "print(sample1_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "495dad80-c3fb-4635-9d67-c7897c601c06",
   "metadata": {
    "id": "495dad80-c3fb-4635-9d67-c7897c601c06",
    "outputId": "feaa2154-2d0c-4632-96f7-c184faba7271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ai', 'is', 'a', 'cs', 'topic']\n",
      "[2, 6, 3, 5, 0]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(sample2)\n",
    "print(tokens)\n",
    "\n",
    "sample2_tokens = [vocab[token] for token in tokens]\n",
    "print(sample2_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10ae1cfe-b2a0-44d1-99ce-f537a74ab151",
   "metadata": {
    "id": "10ae1cfe-b2a0-44d1-99ce-f537a74ab151",
    "outputId": "55d35b21-3194-4a96-daa0-4483ee40645c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Sample 1: tensor([0, 4, 7, 2, 1])\n",
      "Vectorized Sample 2: tensor([2, 6, 3, 5, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def vectorize(text, vocab, seq_len):\n",
    "    tokens = tokenizer(text)\n",
    "    tokens = [vocab[token] for token in tokens]\n",
    "\n",
    "    num_pads = sequence_length - len(tokens)\n",
    "    tokens = tokens[:sequence_length] + [vocab[\"<pad>\"]]*num_pads\n",
    "\n",
    "    return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# Vectorize the samples\n",
    "sequence_length = 5\n",
    "vectorized_sample1 = vectorize(sample1, vocab,\n",
    "                               sequence_length)\n",
    "vectorized_sample2 = vectorize(sample2, vocab,\n",
    "                               sequence_length)\n",
    "\n",
    "print(\"Vectorized Sample 1:\", vectorized_sample1)\n",
    "print(\"Vectorized Sample 2:\", vectorized_sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "897b7f8e-1118-45e5-9b8d-6c31a7fe4113",
   "metadata": {
    "id": "897b7f8e-1118-45e5-9b8d-6c31a7fe4113",
    "outputId": "41167da0-1dce-4c91-9c31-cda7eea8be24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 0, 0, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "sample3 = 'AI topic in CS is difficult'\n",
    "vectorized_sample3 = vectorize(sample3, vocab,\n",
    "                               sequence_length)\n",
    "print(vectorized_sample3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a026fa8-8860-49e1-a621-18596279af5d",
   "metadata": {
    "id": "2a026fa8-8860-49e1-a621-18596279af5d"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7fa0ebb-c8e0-4ef3-9ea1-bdca2c51c888",
   "metadata": {
    "id": "f7fa0ebb-c8e0-4ef3-9ea1-bdca2c51c888",
    "outputId": "0e326aa2-967a-4e04-edcc-cf8af84db437"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1882,  0.5530,  1.6267,  0.7013],\n",
      "        [ 1.7840, -0.8278, -0.2701,  1.3586],\n",
      "        [ 1.0281, -1.9094,  0.3182,  0.4211],\n",
      "        [-1.3083, -0.0987,  0.7647, -0.3680],\n",
      "        [ 0.2293,  1.3255,  0.1318,  2.0501],\n",
      "        [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
      "        [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
      "        [ 0.4309, -1.3067, -0.8823,  1.5977]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 8\n",
    "embed_dim = 4\n",
    "embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "custom_weights = torch.tensor( [[-0.1882,  0.5530,  1.6267,  0.7013],\n",
    "                                [ 1.7840, -0.8278, -0.2701,  1.3586],\n",
    "                                [ 1.0281, -1.9094,  0.3182,  0.4211],\n",
    "                                [-1.3083, -0.0987,  0.7647, -0.3680],\n",
    "                                [ 0.2293,  1.3255,  0.1318,  2.0501],\n",
    "                                [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
    "                                [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
    "                                [ 0.4309, -1.3067, -0.8823,  1.5977]]).float()\n",
    "embedding.weight = nn.Parameter(custom_weights)\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a027791-dc92-4ac7-8fb7-20c68d528a93",
   "metadata": {
    "id": "3a027791-dc92-4ac7-8fb7-20c68d528a93"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59e8091c-b48c-479e-afb4-500f25f75835",
   "metadata": {
    "id": "59e8091c-b48c-479e-afb4-500f25f75835"
   },
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab9d2b8-bc27-45ff-9860-27d5eb74d2b5",
   "metadata": {
    "id": "eab9d2b8-bc27-45ff-9860-27d5eb74d2b5",
    "outputId": "7e4ba43a-8a34-49f4-c781-57b21dd6b631"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1882,  0.5530,  1.6267,  0.7013],\n",
      "        [ 1.7840, -0.8278, -0.2701,  1.3586],\n",
      "        [ 1.0281, -1.9094,  0.3182,  0.4211],\n",
      "        [-1.3083, -0.0987,  0.7647, -0.3680],\n",
      "        [ 0.2293,  1.3255,  0.1318,  2.0501],\n",
      "        [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
      "        [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
      "        [ 0.4309, -1.3067, -0.8823,  1.5977]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "vocab_size = 8\n",
    "embed_dim = 4\n",
    "embedding = nn.Embedding(vocab_size,\n",
    "                         embed_dim)\n",
    "\n",
    "custom_weights = torch.tensor( [[-0.1882,  0.5530,  1.6267,  0.7013],\n",
    "                                [ 1.7840, -0.8278, -0.2701,  1.3586],\n",
    "                                [ 1.0281, -1.9094,  0.3182,  0.4211],\n",
    "                                [-1.3083, -0.0987,  0.7647, -0.3680],\n",
    "                                [ 0.2293,  1.3255,  0.1318,  2.0501],\n",
    "                                [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
    "                                [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
    "                                [ 0.4309, -1.3067, -0.8823,  1.5977]]).float()\n",
    "embedding.weight = nn.Parameter(custom_weights)\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcffa230-ef6c-450d-a781-e9b20f727dc9",
   "metadata": {
    "id": "dcffa230-ef6c-450d-a781-e9b20f727dc9"
   },
   "outputs": [],
   "source": [
    "data = torch.tensor([0, 4, 7, 2, 1], dtype=torch.long)\n",
    "label = torch.tensor([1], dtype=torch.float)\n",
    "\n",
    "x = embedding(data)\n",
    "x = nn.Flatten(0)(x)\n",
    "x = nn.Linear(20, 1)(x)\n",
    "output = nn.Sigmoid()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49429089-131c-414f-94a6-4a2dccd17b3d",
   "metadata": {
    "id": "49429089-131c-414f-94a6-4a2dccd17b3d",
    "outputId": "e39faed5-e929-49f9-cd63-02a2937ab51b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.1872,  0.5520,  1.6277,  0.7023],\n",
      "        [ 1.7850, -0.8288, -0.2711,  1.3596],\n",
      "        [ 1.0291, -1.9084,  0.3172,  0.4221],\n",
      "        [-1.3083, -0.0987,  0.7647, -0.3680],\n",
      "        [ 0.2303,  1.3265,  0.1308,  2.0511],\n",
      "        [ 0.4058, -0.6624, -0.8745,  0.7203],\n",
      "        [ 0.5582,  0.0786, -0.6817,  0.6902],\n",
      "        [ 0.4299, -1.3057, -0.8833,  1.5987]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "loss = nn.BCELoss()(output, label)\n",
    "optimizer = torch.optim.Adam(embedding.parameters())\n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "print(embedding.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e250447-6ebf-4cf2-ae22-19c2c282be1b",
   "metadata": {
    "id": "1e250447-6ebf-4cf2-ae22-19c2c282be1b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
